---
- hosts: aws
  gather_facts: no
  become: yes
  ignore_errors: True
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
#    - name: set up trex.fi DNS64
#      shell: "[ `grep '2001:67c:2b0::4' /etc/systemd/resolved.conf|wc -l` -eq 0 ] && echo 'DNS=2001:67c:2b0::4 2001:67c:2b0::6;' >>/etc/systemd/resolved.conf && rm /etc/resolv.conf && ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf && service systemd-resolved restart"
    - name: Update hosts
      shell: "[ `grep spark /etc/hosts|wc -l` -eq 0 ] && echo '10.0.0.14    spark-master' >> /etc/hosts && echo '10.0.0.5    spark-worker1' >>/etc/hosts && echo '10.0.0.9    postgis' >>/etc/hosts"
    - name: set aws credentials
      shell: "[ `grep AWS_ACCESS_KEY_ID /home/ubuntu/.profile|wc -l` -eq 0 ] && echo 'export AWS_ACCESS_KEY_ID=AWSID' >>/home/ubuntu/.profile && echo 'export AWS_SECRET_ACCESS_KEY=AWSKEY' >>/home/ubuntu/.profile"
#    - name: Update apt source
#      shell: "sed -e 's/us-west-1\\.ec2\\.//g' -i /etc/apt/sources.list"
    - name: Update apt-get repo and cache
      apt: update_cache=yes force_apt_get=yes cache_valid_time=3600
    - name: Upgrade all apt packages
      apt: upgrade=dist force_apt_get=yes
    - name: Check if a reboot is needed for Debian and Ubuntu boxes
      register: reboot_required_file
      stat: path=/var/run/reboot-required get_md5=no
      ###Install prerequites
    - name: Install prerequites
      apt:
        name: ['screen', 'openjdk-8-jre-headless', 'docker-compose', 'apt-transport-https', 'ca-certificates', 'curl', 'gnupg-agent', 'software-properties-common', 'net-tools', 'python3-pip', 'libpq-dev', 'scala']
        state: present
    - name: Add docker repository key
      shell: "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -"
      ignore_errors: True
    - name: Add docker repository
      shell: "add-apt-repository 'deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable'"
      ignore_errors: True
    - name: Update apt-get repo and cache
      apt: update_cache=yes force_apt_get=yes cache_valid_time=3600
    - name: Upgrade all apt packages
      apt: upgrade=dist force_apt_get=yes
    - name: Install prerequites
      apt:
        name: ['docker-ce', 'docker-ce-cli', 'containerd.io']
        state: present
- hosts: spark*
  gather_facts: no
  become: yes
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
    - name: get spark
      unarchive:
        src: https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
        dest: /opt
        remote_src: yes
    - name: rename spark folder
      shell: "mv /opt/spark-2.4.3-bin-hadoop2.7 /opt/spark"
      ignore_errors: True
#    - name: get hadoop
#      unarchive:
#        src: http://apache.mirrors.hoobly.com/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz
#        dest: /opt
#        remote_src: yes
#    - name: rename hadoop folder
#      shell: "mv /opt/hadoop-3.2.1 /opt/hadoop"
#      ignore_errors: True
    - name: setup spark
      #      shell: "[ `grep SPARK /home/ubuntu/.bashrc|wc -l` -eq 0 ] && echo 'export SPARK_HOME=/opt/spark' >> /home/ubuntu/.bashrc && echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' >> /home/ubuntu/.bashrc && echo 'export PYSPARK_PYTHON=/usr/bin/python3' >> /home/ubuntu/.bashrc && mv /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh && echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/hadoop/lib/native' >>/opt/spark/conf/spark-env.sh && chmod +x /opt/spark/conf/spark-env.sh"
      shell: "[ `grep SPARK /home/ubuntu/.bashrc|wc -l` -eq 0 ] && echo 'export SPARK_HOME=/opt/spark' >> /home/ubuntu/.bashrc && echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' >> /home/ubuntu/.bashrc && echo 'export PYSPARK_PYTHON=/usr/bin/python3' >> /home/ubuntu/.bashrc && mv /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh && echo 'export SPARK_MASTER_HOST=spark-master' >> /opt/spark/conf/spark-env.sh && echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64'  >> /opt/spark/conf/spark-env.sh && echo 'export PYSPARK_PYTHON=python3' >> /opt/spark/conf/spark-env.sh && chmod +x /opt/spark/conf/spark-env.sh"
      ignore_errors: True
    - name: Download postgis plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.16/postgresql-42.2.16.jar
        dest: /opt/spark/jars/
    - name: Download postgis plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.16/postgresql-42.2.16.pom
        dest: /opt/spark/jars/
    - name: Download aws plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.7/hadoop-aws-2.7.7.jar
        dest: /opt/spark/jars/
    - name: Download aws plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar
        dest: /opt/spark/jars/
    - name: Install python libraries
      pip:
        name:
          - pyspark
          - holidays
#- hosts: postgis*
#  gather_facts: no
#  become: yes
#  vars:
#    ansible_python_interpreter: /usr/bin/python3
#  tasks:
#    - name: Pull postgis image
#      community.general.docker_image:
#        name: kartoza/postgis
#        source: pull
#    - name: Create postgis data volume
#      community.general.docker_volume:
#        name: postgis
#        driver_options:
#          type: ext4
#          device: /dev/mapper/smartrider-data
#    - name: set up postgis docker
#      shell: "docker pull kartoza/postgis; docker volume create --opt type=ext4 --opt device=/dev/mapper/smartrider-data postgis"
#    - name: Add user to docker group
#      shell: "usermod -aG docker ubuntu"
