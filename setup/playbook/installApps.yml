---
- hosts: aws
  gather_facts: no
  become: yes
  ignore_errors: True
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
    - name: set up trex.fi DNS64
      shell: "[ `grep '2001:67c:2b0::4' /etc/systemd/resolved.conf|wc -l` -eq 0 ] && echo 'DNS=2001:67c:2b0::4 2001:67c:2b0::6' >>/etc/systemd/resolved.conf && rm /etc/resolv.conf && ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf && service systemd-resolved restart"
    - name: Update hosts
      shell: "[ `grep spark /etc/hosts|wc -l` -eq 0 ] && echo '10.0.0.8    spark-master' >> /etc/hosts && echo '10.0.0.7    spark-worker1' >>/etc/hosts && echo '10.0.0.13    spark-worker2' >>/etc/hosts && echo '10.0.0.10    spark-worker3' >>/etc/hosts && echo '10.0.0.9    postgis' >>/etc/hosts"
    - name: set aws credentials
      shell: "[ `grep AWS_ACCESS_KEY_ID /home/ubuntu/.profile|wc -l` -eq 0 ] && echo 'export AWS_ACCESS_KEY_ID=AWSID' >>/home/ubuntu/.profile && echo 'export AWS_SECRET_ACCESS_KEY=AWSKEY' >>/home/ubuntu/.profile"
    - name: Update apt source
      shell: "sed -e 's/us-west-1\\.ec2\\.//g' -i /etc/apt/sources.list"
    - name: Update apt-get repo and cache
      apt: update_cache=yes force_apt_get=yes cache_valid_time=3600
    - name: Upgrade all apt packages
      apt: upgrade=dist force_apt_get=yes
    - name: Check if a reboot is needed for Debian and Ubuntu boxes
      register: reboot_required_file
      stat: path=/var/run/reboot-required get_md5=no
      ###Install prerequites
    - name: Install prerequites
      apt:
        name: ['screen', 'default-jre', 'net-tools', 'python3-pip', 'libpq-dev', 'docker-compose', 'apt-transport-https', 'ca-certificates', 'curl', 'gnupg-agent', 'software-properties-common']
        state: present
    - name: Add docker repository key
      shell: "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -"
    - name: Add docker repository
      shell: "add-apt-repository 'deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable'"
    - name: Update apt-get repo and cache
      apt: update_cache=yes force_apt_get=yes cache_valid_time=3600
    - name: Upgrade all apt packages
      apt: upgrade=dist force_apt_get=yes
    - name: Install Docker
      apt:
        name: ['docker-ce', 'docker-ce-cli', 'containerd.io']
        state: present
    - name: Add user to docker group
      shell: "usermod -aG docker AWSUSER"
- hosts: spark*
  gather_facts: no
  become: yes
  ignore_errors: True
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
    - name: get spark
      unarchive:
        src: http://apache.mirrors.hoobly.com/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz
        dest: /opt
        remote_src: yes
    - name: rename spark folder
      shell: "mv /opt/spark-3.0.1-bin-hadoop3.2 /opt/spark"
    - name: get hadoop
      unarchive:
        src: http://apache.mirrors.hoobly.com/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz
        dest: /opt
        remote_src: yes
    - name: rename hadoop folder
      shell: "mv /opt/hadoop-3.2.1 /opt/hadoop"
    - name: setup spark
      shell: "[ `grep SPARK /home/ubuntu/.bashrc|wc -l` -eq 0 ] && echo 'export SPARK_HOME=/opt/spark' >> /home/ubuntu/.bashrc && echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' >> /home/ubuntu/.bashrc && echo 'export PYSPARK_PYTHON=/usr/bin/python3' >> /home/ubuntu/.bashrc && mv /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh && echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/hadoop/lib/native' >>/opt/spark/conf/spark-env.sh && echo 'export SPARK_MASTER_HOST=spark-master' >> /opt/spark/conf/spark-env.sh && echo 'export JAVA_HOME=/usr/lib/jvm/default-java' >> /opt/spark/conf/spark-env.sh && echo 'export SPARK_WORKER_CORES=6' >> /opt/spark/conf/spark-env.sh && chmod +x /opt/spark/conf/spark-env.sh"
    - name: Download postgis plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.16/postgresql-42.2.16.jar
        dest: /opt/spark/jars/
    - name: Download postgis plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.16/postgresql-42.2.16.pom
        dest: /opt/spark/jars/
    - name: Download aws plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar
        dest: /opt/spark/jars/
    - name: Download aws plugins for spark
      get_url:
        url: https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.876/aws-java-sdk-bundle-1.11.876.jar
        dest: /opt/spark/jars/
    - name: Install python libraries
      pip:
        name:
          - pyspark
          - holidays
- hosts: postgis*
  gather_facts: no
  become: yes
  ignore_errors: True
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
    - name: set up postgis docker
      shell: "docker pull kartoza/postgis; docker volume create --opt type=ext4 --opt device=/dev/mapper/smartrider-data postgis"
    - name: Install python libraries
      pip:
        name:
          - dash
          - dash-leaflet
          - holidays
          - psycopg2
          - osmnx
